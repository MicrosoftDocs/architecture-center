This article describes infrastructure design where the challenge extends beyond identifying the right components and resources. The real complexity lies in how these elements are layered and interconnected while creating a unified visual representation that caters to both technical and non-technical audiences. A key to overcome this challenge is understanding the audience. The solution is to develop a design that balances technical depth with accessibility. This approach ensures clarity for stakeholders across all levels of expertise.

## Architecture
![Reference Blueprint.](_images/data-ai-ml-end-to-end-reference-flow.svg)
*Download a [Visio file](https://arch-center.azureedge.net/data-ai-ml-end-to-end-reference-flow.vsdx) of this architecture.*

The reference blueprint is strategically designed to accommodate multiple levels of understanding. At the highest level, stakeholders can grasp the overall flow through labeled boxes and numerical indicators. For those seeking more detailed insights, a deeper dive into the individual components and resources is possible, ensuring the blueprint is both comprehensive and accessible. By creating such a flexible yet coherent design, this reference blueprint can serve as a guide for infrastructure planning, facilitating communication between technical and non-technical audiences alike.

### Infrastructure Requirements
[Azure Data Factory]() is used for ingesting data from various internal and external sources, while Azure Databricks handles ETL processes, data processing, and the setup of machine learning pipelines for training, enrichment and evaluation. [Azure Data Lake]() serves as the data landing zone, supporting a [medallion lakehouse architecture](https://learn.microsoft.com/en-us/azure/databricks/lakehouse/medallion) to organize data and ensure quality. To enable third-party data sharing, [Delta Sharing]() is utilized. [Azure App Service]() is used for deploying ML-managed endpoints and handling backend orchestration along with supporting the front-end application. Everytime after defined reulgar intervals, ML pipeline is triggered and the processed, cleaned, and aggregated data as output from ML pipeline is stored in [Azure SQL Database](). This approach minimizes the need for real-time inference, which can be costly. The orhcestration engine serving the front-end application can simply pull the processed output from ML pipeline via SQL Database rather than real-time inference. [Azure Blob Storage]() is used for storing static files and metadata. [Azure Open AI]() provides generative content capabilities, while [Azure AI Search]() helps in contextualization of this generative content and supports Retrieval-Augmented Generation (RAG). For storing historical data to ensure persistence in interactions with Co-Pilot or chat, [Cosmos DB]() is used. While Cosmos DB is often avoided due to its cost, the pricing is based on Request Units (RUs) consumption. RUs consumption can be significantly reduced with [optimizing strategies](https://learn.microsoft.com/en-us/azure/cosmos-db/optimize-cost-reads-writes) including proper indexing, partitioning, and most importantly utilizing point reads. [Traffic Manager]() functions as a load balancer, routing traffic to app service instances across multiple regions based on proximity, ensuring efficient performance and scalability. While the [Azure Data Factory]() is used for batch ingestion, incase of real time data streaming [EventHub]() can be an option and [Azure Functions]() for processing real time data stream. 

*Unlike batch processing, where the ML pipeline can be triggered at regular intervals, triggering the pipeline for each event in an event stream may not be optimal. This is because there may not be enough data in each individual event to retrain the models and produce a meaningful output that differs significantly from the previous event.*

### Security Requirements
A new [Subscription]() and [Resource Group]() are required to organize and manage the resources. [Azure Key Vault]() is essential for securely storing and retrieving sensitive data, such as keys, on demand. [Managed Identities]() allow services like Azure DevOps to connect with Azure resources for deployment without the need for shared keys, ensuring a more secure and streamlined process. [Entra ID]() enables Microsoft single sign-on, simplifying authentication and access management. To enhance security, a Private Endpoint and Subnet can be configured to limit external access to core services like AI Search and Open AI. Firewall and Application Gateway can be configured to provide a safety layer for app deployments, safeguarding them from potential web threats and ensuring secure access to resources.

### DevOps Requirements
Infrastructure-as-code (IaC) via Terraform is used for auto provisioning Azure resources, ensuring a consistent and repeatable infrastructure setup. [Azure Container Registry (ACR)]() facilitates container-based deployments. For continuous integration and deployment (CI/CD), [Azure DevOps]() is utilized to configure YAML files to define continuous integration and continuous deployment pipelines. [Azure Oryx](), Microsoft's open-source build tool, automates app building and deployment for services like Azure App Service and Static Web Apps, making the deployment process faster and more efficient.

### Post-Production Monitoring and SRE
[Application Insights]() is used for logging all the required insights, telemetry and logging events. [Azure Monitor]() provides comprehensive monitoring of all the resources provisioned within a subscription and resource group. With [Cost Management]() alerts can be setup based on predefined thresholds, helping to manage and optimize costs by providing visibility and control.

### Decision Rationale
- Infrastructure-as-Code (IaC) - [BICEP]() vs [Terraform](): Terraform supports multiple cloud providers, offering flexibility for potential future cloud expansion. Moreover, the built-in state management in Terraform enables tracking infrastructure changes effectively.
- Azure Fabric vs Azure Data Factory and Azure Databricks: Azure Data Factory provides powerful orchestration and scheduling capabilities for complex workflows. [Azure Fabric]() has a broader scope over Azure Data Factory and Azure Databricks.

