---
title: Overview of the performance efficiency pillar
description: Describes the performance efficiency pillar
author: v-aangie
ms.date: 10/23/2020
ms.topic: conceptual
ms.service: architecture-center
ms.subservice: well-architected
ms.custom:
  - overview
---

# Overview of the performance efficiency pillar

Performance efficiency is the ability of your workload to scale to meet the demands placed on it by users in an efficient manner. Before the cloud became popular, when it came to planning how a system would handle increases in load, many organizations intentionally provisioned workloads to be oversized to meet business requirements. This might make sense in on-premises environments because it ensured *capacity* during peak usage. [Capacity](/azure/api-management/api-management-capacity#what-is-capacity) reflects resource availability (CPU and memory). This was a major consideration for processes that would be in place for a number of years.

Just as you needed to anticipate increases in load in on-premises environments, you need to anticipate increases in cloud environments to meet business requirements. One difference is that you may no longer need to make long-term predictions for anticipated changes to ensure that you will have enough capacity in the future. Another difference is in the approach used to manage performance.

## What is scalability and why is it important?

An important consideration in achieving performance efficiency is to consider how your application scales and to implement PaaS offerings that have built-in scaling operations. *Scalability* is the ability of a system to handle increased load. Services covered by [Azure Autoscale](/azure/azure-monitor/platform/autoscale-overview)<!--replace LINK with new Autoscaling--> can scale automatically to match demand to accommodate workload. They will scale out to ensure capacity during workload peaks and scaling will return to normal automatically when the peak drops.

In the cloud, the ability to take advantage of scalability depends on your infrastructure and services. Some platforms, such as Kubernetes, were built with scaling in mind. Virtual machines, on the other hand, may not scale as easily although scale operations are possible. With virtual machines, you may want to plan ahead to avoid scaling infrastructure in the future to meet demand. Another option is to select a different platform such as Azure virtual machines scale sets.

When using scalability, you need only predict the current average and peak times for your workload. Payment plan options allow you to manage this. You pay either per minute or per-hour depending on the service for a designated time period.

## Principles

Follow these principles to guide you through improving performance efficiency:

- **Performance testing** - It is imperative that any development efforts undergo continuous performance testing to ensure that any changes to the codebase does not negatively affect the application's performance. When conducting performance testing, it is important that you first establish baselines for your application and, then, establish a regular cadence for testing. This cadence for testing can be a regularly scheduled event or part of a CI build pipeline. But, regardless of how performance testing is conducted, it should be considered a requirement within the scope of development.

  - **Establish baselines** - Baselines help to establish the current efficiency of your application and its supporting infrastructure. Baselines can provide a good bearing for improvements in your application's performance and helps you determine if the application is meeting the business's KPIs. Baselines can be created for any application regardless of its maturity&mdash;a new project or an application that has been running in production for years. No matter when you establish the baseline, performance during continued development can be continuously measured against it. When code and/or infrastructure changes, the effect on performance can be actively measured.

  - **Load & stress testing** - Besides standard performance testing and the constant measurements against your baselines, there are two additional flavors of testing&mdash;load and stress testing. Load testing measures your application's performance under predetermined amounts of load. Stress testing measures the _maximum_ load your application and its infrastructure can support before it buckles.

    Load testing takes places in stages of load. These stages are usually measured by virtual users (VUs) or simulated requests, and the stages happen over given intervals. Load testing provides insights into how and when your application needs to scale in order to continue to meet your SLA to your customers (whether internal or external). Load testing can also be useful for determining latency across distributed applications and microservices.

  - **Identifying bottlenecks** - Bottlenecks are areas within your application that can hinder performance, and the bottleneck typically worsens as load increases. Bottlenecks can be the result of deficiencies in code or misconfiguration of a service.

  - **Improvement options** - The first step for determining options for improvement is to ensure that you are capturing telemetry throughout your application. [Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview) provides some great telemetry out of the box, and you can customize what is captured for even greater visibility. Next, you may want to compare your code to proven architectures in the [Cloud Design Patterns](../../patterns/index-patterns). By referencing the design patterns, you can avoid common mistakes by developers who are deploying applications into the cloud. Finally, you may consider other Azure services that may be more appropriate for your objectives. While Azure has many services that seem to overlap in capabilities, often there are specific use-cases for which the services are designed.
  
- **Monitoring** - Lack of monitoring new servicesâ€‹ and the health of current workloads are major inhibitors in workload quality. The overall monitoring strategy should take into consideration not only scalability, but resiliency (infrastructure, application, and dependent services) and application performance as well. For purposes of scalability, looking at the metrics would allow you to provision resources dynamically and scale with demand.

  - **Data-driven processes** - Performance testing should always be based on data captured from repeatable processes. In order to understand how an application's performance is affected by code and infrastructure changes, data must be kept and monitored. Additionally, it is important to understand how performance has changed _over time_, not just compared to the last measurement taken. It is often helpful to store such data in a time-series database (TSDB) and then view the data from an operational dashboard. An [Azure Data Explorer cluster](https://azure.microsoft.com/services/data-explorer/) is a powerful TSDB that can store any schema of data, including performance test metrics. [Grafana](https://grafana.com/), an open source platform for observability dashboards, can then be leveraged to query your Azure Data Explorer cluster to view performance trends in your application.

  - **Troubleshooting** - Troubleshooting an application's performance requires continuous monitoring and reliable investigation. Issues in performance can arise from database queries, connectivity between services, under-provision resources, or memory leaks in code. Application telemetry and profiling can be useful tools for troubleshooting your application. Additionally, network traffic capturing tools, such as [Azure Network Watcher](https://docs.microsoft.com/azure/network-watcher/network-watcher-monitoring-overview), can be extremely helpful.

  - **Resolution planning** - Resolving performance issues requires time and patience&mdash;not just in discovery and investigation, but also in resolution. Code enhancements may be accomplished by deploying a new build, but enhancements to infrastructure may involve many teams. Some services may require updated configurations while others may need to be deprecated in favor of more-appropriate solutions. Regardless, it is critical that you understand the scope of your planned resolution so that all necessary stakeholders are informed.

- **Capacity planning** - When performance testing, the business must communicate any fluctuation in expected load. Load can be impacted by world events, such as political, economic, or weather changes; by marketing initiatives, such as sales or promotions; or, by seasonal events, such as holidays. You should test variations of load prior to events, including unexpected ones, to ensure that your application can scale. Additionally, you should ensure that all regions can adequately scale to support total load, should one region fail.

  - **Technology & SKU service limits** - For disaster recovery, it is important that you choose [_paired regions_](https://docs.microsoft.com/azure/best-practices-availability-paired-regions) for deploying your application. This is to ensure that your application operates at maximum uptime. Additionally, it is critical that your chosen regions support the necessary service SKUs and limits to support load if one region should fail. In the event that a single region fails, your services should be able to scale to the appropriate levels for accommodating increased demand from the rerouted requests. Your performance testing should occasionally include testing during a simulated failover to ensure application efficiency under those conditions. If it is determined that a service's limits in a single region cannot support the increased load, additional services should be deployed in your regions, or additional regions should be added to your load balancer.

  - **SLAs** - When determining which services you will use within your application, you should also consider the SLAs of those services as their SLAs can affect your application's performance. When the _functional_ capabilities of two services overlap, then the SLAs of those services could be the determining factor why one is chosen over the other.

  - **Costs** - Costs can be an additional factor in performance efficiency. Surprisingly, however, greater costs don't necessarily always mean increased performance. But, when observing how close your application comes to meeting business KPIs, you must consider how much improvement will be realized in the application if costs are increased. Then, you must evaluate if the cost-benefit is worth the investment. Sometimes, small changes in the application logic, caching, or adding an index to a database can dramatically improve performance while not affecting costs whatsoever (certain improvements can even help reduce costs).

- **Distributed architecture** - While the cloud is leveraged for hosting many monolithic applications, it is more common to see cloud deployments consist of components distributed across various services. Troubleshooting monolithic applications often requires only one or two lenses&mdash;the application and the database. However, distributed architectures are much more complex and can require many areas of expertise. To adequately monitor performance, it is critical that as much telemetry is captured throughout the application&mdash;across all services&mdash;as possible. Additionally, your team should be equipped with the necessary expertise to troubleshoot all services in your architecture. On occasion, it may be advantageous to use one service over another simply because your team is better equipped to support it. Then, as your team's expertise grows, you can incorporate other technologies.

  - **Scale** - For monolithic applications, scale is very two-dimensional as the applications usually consist of a group of application servers, maybe some web front ends (WFEs), and some database servers. Uncovering bottlenecks is much simpler (though resolving them could still require some considerable effort). For distributed applications, required effort for ensuring performance efficiency is increased exponentially. You must consider each application, its supporting service, and the latency between all application layers. 
  
    Performance efficiency is a complex mixture of applications and infrastructure (IaaS and PaaS). First, you must ensure that all services can scale to support load and that one service will not prove to be a bottleneck. Second, while performance testing, you may realize that certain services should scale under different load conditions (versus scaling all services uniformly). Monitoring all of your services and their infrastructure can help fine-tune your application for optimal performance.

  - **Anti-pattern avoidance** - A performance antipattern is a common practice that is likely to cause scalability problems when an application is under pressure. For example, you can have an application that behaves as expected during performance testing. However, when it is released to production and starts to handle live workloads, performance decreases. Scalability problems such as rejecting user requests, stalling, or throwing exceptions may arise. To learn how to identify and fix these antipatterns, see [Performance antipatterns for cloud applications](../../antipatterns/index.md).

  - **Fault-handling** - If a service within your architecture fails, how will it affect your application's overall performance? Is the error transient, thus allowing your application to continue to function; or, will the application experience a critical failure? If the error is transient, does your application experience a decrease in performance? Resiliency plays a huge role in performance efficiency as the failure of any service may impact your application's ability to meet your business's KPIs or, worse, scale to meet current load. Chaos testing&mdash;the introduction of random failures within your infrastructure&mdash;against your application can help determine how well your application continues to perform under varying stages of load. To learn more about chaos testing, see [Advancing resilience through chaos engineering and fault injection](https://azure.microsoft.com/blog/advancing-resilience-through-chaos-engineering-and-fault-injection/).

## Next steps

- Performance efficiency impacts the entire architecture spectrum. Bridge gaps in you knowledge of Azure by reviewing the 5 pillars in the [Microsoft Azure Well-Architected Framework](../index.md).

- To assess your workload using the tenets found in the Microsoft Azure Well-Architected Framework, see the [Microsoft Azure Well-Architected Review](/assessments/?id=azure-architecture-review&mode=pre-assessment).
