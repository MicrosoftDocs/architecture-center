---
ms.author: dastanfo
author: david-stanford
ms.date: 10/16/2019
ms.topic: article
ms.service: architecture-center
ms.subservice: cloud-design-principles
ms.uid: dfe40589-8d10-40c4-85cd-e15ab0ce3b19
ms.assessment_question: Automatically scale when load increses
---
## Autoscaling

Many of the compute offerings in Azure offer auto-scale to ensure the right amount of resources are available mapping to service user demand. With auto-scale, your service can scale out to new compute instances during busy periods and scale back in during silent periods. This is critical for both the service user experience as well as offering cost savings for the service implementor. The way it works is metrics are collected for the resource (i.e. cpu, memory utilization) and the application (requests queued, requests per second). Rules can then be created off those metrics and/or time schedules to add/remove instances depending on how the rule evaluates.

Many of the common Azure compute offerings support auto-scale. An App Services App Plan allows auto-scale rules to be set for scale-out and scale-in. Azure Kubernetes Service (AKS) offers two levels of auto-scale. Firstly, AKS provides horizontal auto-scale can be enabled on service containers to add more or less pod instances within the cluster. Secondly, AKS provides a cluster auto-scale on the agent VM instances running an agent node-pool to add more ore remove VM instances dynamically. Service Fabric has similar offerings and VM Scale Sets offers auto-scale capabilities for true IaaS scenarios. In addition, Azure App Gateway and Azure API Management are PaaS offerings for ingress services that also enable auto-scale. And lastly, Azure Functions, Azure Logic Apps, and App Services offer serverless pay-per-use consumption modelling that inherently provide auto-scale.

Each service documents its auto-scale capabilities. A general discussion on Azure platform auto-scale here: <https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-overview>
